{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3 ... 2 2 2]\n",
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "2257\n",
      "2257\n",
      "-----\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "-----\n",
      "comp.graphics\n",
      "-----\n",
      "[1 1 3 3 3 3 3 2 2 2]\n",
      "-----\n",
      "comp.graphics\n",
      "comp.graphics\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "soc.religion.christian\n",
      "sci.med\n",
      "sci.med\n",
      "sci.med\n",
      "-----\n",
      "  (0, 230)\t1\n",
      "  (0, 12541)\t1\n",
      "  (0, 3166)\t1\n",
      "  (0, 14085)\t1\n",
      "  (0, 20459)\t1\n",
      "  (0, 35416)\t1\n",
      "  (0, 3062)\t1\n",
      "  (0, 2326)\t2\n",
      "  (0, 177)\t2\n",
      "  (0, 31915)\t1\n",
      "  (0, 33572)\t1\n",
      "  (0, 9338)\t1\n",
      "  (0, 26175)\t1\n",
      "  (0, 4378)\t1\n",
      "  (0, 17556)\t1\n",
      "  (0, 32135)\t1\n",
      "  (0, 15837)\t1\n",
      "  (0, 9932)\t1\n",
      "  (0, 32270)\t1\n",
      "  (0, 18474)\t1\n",
      "  (0, 27836)\t1\n",
      "  (0, 5195)\t1\n",
      "  (0, 12833)\t2\n",
      "  (0, 25337)\t1\n",
      "  (0, 25361)\t1\n",
      "  :\t:\n",
      "  (2256, 6430)\t1\n",
      "  (2256, 24052)\t1\n",
      "  (2256, 22270)\t1\n",
      "  (2256, 35638)\t2\n",
      "  (2256, 32233)\t1\n",
      "  (2256, 35157)\t1\n",
      "  (2256, 4938)\t1\n",
      "  (2256, 34923)\t1\n",
      "  (2256, 5698)\t1\n",
      "  (2256, 27031)\t2\n",
      "  (2256, 14601)\t1\n",
      "  (2256, 21322)\t1\n",
      "  (2256, 7766)\t1\n",
      "  (2256, 12626)\t2\n",
      "  (2256, 9338)\t1\n",
      "  (2256, 17556)\t1\n",
      "  (2256, 32270)\t1\n",
      "  (2256, 18474)\t2\n",
      "  (2256, 23610)\t2\n",
      "  (2256, 587)\t1\n",
      "  (2256, 20253)\t1\n",
      "  (2256, 32142)\t6\n",
      "  (2256, 23915)\t1\n",
      "  (2256, 31077)\t1\n",
      "  (2256, 14887)\t1\n",
      "(2257, 35788)\n",
      "-----\n",
      "4690\n",
      "-----\n",
      "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False,\n",
      "         use_idf=False)\n",
      "-----\n",
      "  (0, 230)\t0.07537783614444091\n",
      "  (0, 12541)\t0.07537783614444091\n",
      "  (0, 3166)\t0.07537783614444091\n",
      "  (0, 14085)\t0.07537783614444091\n",
      "  (0, 20459)\t0.07537783614444091\n",
      "  (0, 35416)\t0.07537783614444091\n",
      "  (0, 3062)\t0.07537783614444091\n",
      "  (0, 2326)\t0.15075567228888181\n",
      "  (0, 177)\t0.15075567228888181\n",
      "  (0, 31915)\t0.07537783614444091\n",
      "  (0, 33572)\t0.07537783614444091\n",
      "  (0, 9338)\t0.07537783614444091\n",
      "  (0, 26175)\t0.07537783614444091\n",
      "  (0, 4378)\t0.07537783614444091\n",
      "  (0, 17556)\t0.07537783614444091\n",
      "  (0, 32135)\t0.07537783614444091\n",
      "  (0, 15837)\t0.07537783614444091\n",
      "  (0, 9932)\t0.07537783614444091\n",
      "  (0, 32270)\t0.07537783614444091\n",
      "  (0, 18474)\t0.07537783614444091\n",
      "  (0, 27836)\t0.07537783614444091\n",
      "  (0, 5195)\t0.07537783614444091\n",
      "  (0, 12833)\t0.15075567228888181\n",
      "  (0, 25337)\t0.07537783614444091\n",
      "  (0, 25361)\t0.07537783614444091\n",
      "  :\t:\n",
      "  (2256, 6430)\t0.07216878364870323\n",
      "  (2256, 24052)\t0.07216878364870323\n",
      "  (2256, 22270)\t0.07216878364870323\n",
      "  (2256, 35638)\t0.14433756729740646\n",
      "  (2256, 32233)\t0.07216878364870323\n",
      "  (2256, 35157)\t0.07216878364870323\n",
      "  (2256, 4938)\t0.07216878364870323\n",
      "  (2256, 34923)\t0.07216878364870323\n",
      "  (2256, 5698)\t0.07216878364870323\n",
      "  (2256, 27031)\t0.14433756729740646\n",
      "  (2256, 14601)\t0.07216878364870323\n",
      "  (2256, 21322)\t0.07216878364870323\n",
      "  (2256, 7766)\t0.07216878364870323\n",
      "  (2256, 12626)\t0.14433756729740646\n",
      "  (2256, 9338)\t0.07216878364870323\n",
      "  (2256, 17556)\t0.07216878364870323\n",
      "  (2256, 32270)\t0.07216878364870323\n",
      "  (2256, 18474)\t0.14433756729740646\n",
      "  (2256, 23610)\t0.14433756729740646\n",
      "  (2256, 587)\t0.07216878364870323\n",
      "  (2256, 20253)\t0.07216878364870323\n",
      "  (2256, 32142)\t0.43301270189221935\n",
      "  (2256, 23915)\t0.07216878364870323\n",
      "  (2256, 31077)\t0.07216878364870323\n",
      "  (2256, 14887)\t0.07216878364870323\n",
      "-----\n",
      "(2257, 35788)\n",
      "-----\n",
      "  (0, 14887)\t0.016797806021219684\n",
      "  (0, 29022)\t0.1348710554299733\n",
      "  (0, 8696)\t0.314400065528974\n",
      "  (0, 4017)\t0.12491817585060791\n",
      "  (0, 33256)\t0.11819702490105698\n",
      "  (0, 21661)\t0.1962279892331408\n",
      "  (0, 9031)\t0.3841803935867984\n",
      "  (0, 31077)\t0.016797806021219684\n",
      "  (0, 9805)\t0.21567205914741705\n",
      "  (0, 17366)\t0.0744441018788533\n",
      "  (0, 32493)\t0.07283773941616518\n",
      "  (0, 16916)\t0.17358472047671197\n",
      "  (0, 19780)\t0.24645540709354397\n",
      "  (0, 17302)\t0.18626015109199115\n",
      "  (0, 23122)\t0.036374916362300114\n",
      "  (0, 25663)\t0.034290706362898604\n",
      "  (0, 16881)\t0.0360441471878483\n",
      "  (0, 16082)\t0.11382738609462074\n",
      "  (0, 23915)\t0.017762318563562172\n",
      "  (0, 32142)\t0.08865416253721688\n",
      "  (0, 33597)\t0.06567578043186388\n",
      "  (0, 20253)\t0.016864892977128034\n",
      "  (0, 587)\t0.05966162012870271\n",
      "  (0, 12051)\t0.037793189755988436\n",
      "  (0, 5201)\t0.04316199700711876\n",
      "  :\t:\n",
      "  (2256, 13740)\t0.08503348972488488\n",
      "  (2256, 14662)\t0.10675645600140808\n",
      "  (2256, 20201)\t0.07380572206614645\n",
      "  (2256, 12443)\t0.5533848656066114\n",
      "  (2256, 30325)\t0.2851629991538151\n",
      "  (2256, 4610)\t0.09276072426248369\n",
      "  (2256, 33844)\t0.09841352581656573\n",
      "  (2256, 17354)\t0.10256037122854149\n",
      "  (2256, 26998)\t0.1016498753357488\n",
      "  (2256, 20277)\t0.1016498753357488\n",
      "  (2256, 20695)\t0.10256037122854149\n",
      "  (2256, 20702)\t0.08369317190645711\n",
      "  (2256, 9649)\t0.09916899209319777\n",
      "  (2256, 9086)\t0.10561084702611609\n",
      "  (2256, 26254)\t0.09330694515090646\n",
      "  (2256, 17133)\t0.19682705163313147\n",
      "  (2256, 4490)\t0.11395377721095844\n",
      "  (2256, 13720)\t0.09699270546460859\n",
      "  (2256, 5016)\t0.12302132956698501\n",
      "  (2256, 9632)\t0.11395377721095844\n",
      "  (2256, 11824)\t0.12028503503707107\n",
      "  (2256, 29993)\t0.12302132956698501\n",
      "  (2256, 1298)\t0.12625767908616806\n",
      "  (2256, 2375)\t0.12625767908616806\n",
      "  (2256, 3921)\t0.13532523144219463\n",
      "(2257, 35788)\n",
      "-----\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "-----\n",
      "[3 1]\n",
      "'God is love' => soc.religion.christian\n",
      "'OpenGL on the GPU is fast' => comp.graphics\n",
      "-----\n",
      "Pipeline(memory=None,\n",
      "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n",
      "[3 1]\n",
      "-----\n",
      "0.8348868175765646\n",
      "-----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:117: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9127829560585885\n",
      "-----\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism       0.95      0.81      0.87       319\n",
      "         comp.graphics       0.88      0.97      0.92       389\n",
      "               sci.med       0.94      0.90      0.92       396\n",
      "soc.religion.christian       0.90      0.95      0.93       398\n",
      "\n",
      "           avg / total       0.92      0.91      0.91      1502\n",
      "\n",
      "[[258  11  15  35]\n",
      " [  4 379   3   3]\n",
      " [  5  33 355   3]\n",
      " [  5  10   4 379]]\n"
     ]
    }
   ],
   "source": [
    "# -*-coding:utf-8-*-\n",
    "\n",
    "# ----------------------\n",
    "#   Author：kevinelstri\n",
    "#   Datetime:2017.2.21\n",
    "# ----------------------\n",
    "\n",
    "# -----------------------\n",
    "#   Working With Text Data  文本数据处理\n",
    "#   http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "# -----------------------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "    这个指南的目的是在一个实际任务上探索scikit-learn的主要工具，在二十个不同的主题上分析一个文本集合。\n",
    "    在这一节中，可以看到：\n",
    "        1、加载文本文件和类别\n",
    "        2、适合机器学习的特征向量提取\n",
    "        3、训练线性模型进行分类\n",
    "        4、使用网格搜索策略，找到一个很好的配置的特征提取组件和分类器\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    1、Loading the 20 newsgroups dataset 加载20个新闻组数据集\n",
    "    为了获得更快的执行时间为第一个例子，我们将工作在部分数据集只有4个类别的数据集中：\n",
    "\"\"\"\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "print (twenty_train.target)\n",
    "print (twenty_train.target_names)  # 训练集中类别的名字，这里只有四个类别\n",
    "print (len(twenty_train.data))  # 训练集中数据的长度\n",
    "print (len(twenty_train.filenames))  # 训练集文件名长度\n",
    "print ('-----')\n",
    "print (\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
    "print ('-----')\n",
    "print (twenty_train.target_names[twenty_train.target[0]])\n",
    "print ('-----')\n",
    "print (twenty_train.target[:10])  # 前十个的类别\n",
    "print ('-----')\n",
    "for t in twenty_train.target[:10]:\n",
    "    print (twenty_train.target_names[t])  # 类别的名字\n",
    "print ('-----')\n",
    "\"\"\"\n",
    "    2、Extracting features from text files 从文本文件中提取特征\n",
    "    为了在文本文件中使用机器学习算法，首先需要将文本内容转换为数值特征向量\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Bags of words 词袋\n",
    "    最直接的方式就是词袋表示法\n",
    "        1、为训练集的任何文档中的每个单词分配一个固定的整数ID（例如通过从字典到整型索引建立字典）\n",
    "        2、对于每个文档，计算每个词出现的次数，并存储到X[i,j]中。\n",
    "\n",
    "    词袋表示：n_features 是语料中不同单词的数量，这个数量通常大于100000.\n",
    "    如果 n_samples == 10000，存储X的数组就需要10000*10000*4byte=4GB,这么大的存储在今天的计算机上是不可能实现的。\n",
    "    幸运的是，X中的大多数值都是0，基于这种原因，我们说词袋是典型的高维稀疏数据集，我们可以只存储那些非0的特征向量。\n",
    "    scipy.sparse 矩阵就是这种数据结构，而scikit-learn内置了这种数据结构。\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Tokenizing text with scikit-learn 使用scikit-learn标记文本\n",
    "    文本处理、分词、过滤停用词都在这些高级组件中，能够建立特征字典并将文档转换成特征向量。\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # sklearn中的文本特征提取组件中，导入特征向量计数函数\n",
    "\n",
    "count_vect = CountVectorizer()  # 特征向量计数函数\n",
    "X_train_counts = count_vect.fit_transform(twenty_train.data)  # 对文本进行特征向量处理\n",
    "print (X_train_counts)  # 特征向量和特征标签\n",
    "print (X_train_counts.shape)  # 形状\n",
    "print ('-----')\n",
    "\n",
    "\"\"\"\n",
    "    CountVectorizer支持计算单词或序列的N-grams，一旦合适，这个向量化就可以建立特征词典。\n",
    "    在整个训练预料中，词汇中的词汇索引值与其频率有关。\n",
    "\"\"\"\n",
    "print (count_vect.vocabulary_.get(u'algorithm'))\n",
    "print ('-----')\n",
    "\n",
    "\"\"\"\n",
    "    From occurrences to frequencies 从事件到频率\n",
    "    计数是一个好的开始，但是也存在一个问题：较长的文本将会比较短的文本有很高的平均计数值，即使他们所表示的话题是一样的。\n",
    "    为了避免潜在的差异，它可以将文档中的每个单词出现的次数在文档的总字数的比例：这个新的特征叫做词频：tf\n",
    "    tf-idf:词频-逆文档频率\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  # sklearn中的文本特征提取组件中，导入词频统计函数\n",
    "\n",
    "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)  # 建立词频统计函数,注意这里idf=False\n",
    "print (tf_transformer)  # 输出函数属性 TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False, use_idf=False)\n",
    "print ('-----')\n",
    "X_train_tf = tf_transformer.transform(X_train_counts)  # 使用函数对文本文档进行tf-idf频率计算\n",
    "print (X_train_tf)\n",
    "print ('-----')\n",
    "print (X_train_tf.shape)\n",
    "print ('-----')\n",
    "\"\"\"\n",
    "    在上面的例子中，使用fit()方法来构建基于数据的预测器，然后使用transform()方法来将计数矩阵用tf-idf表示。\n",
    "    这两个步骤可以通过跳过冗余处理，来更快的达到相同的最终结果。\n",
    "    这些可以通过使用fit_transform()方法来实现：\n",
    "\"\"\"\n",
    "tfidf_transformer = TfidfTransformer()  # 这里使用的是tf-idf\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (X_train_tfidf)\n",
    "print (X_train_tfidf.shape)\n",
    "print ('-----')\n",
    "\"\"\"\n",
    "    Training a classifier 训练一个分类器\n",
    "    既然已经有了特征，就可以训练分类器来试图预测一个帖子的类别，先使用贝叶斯分类器，贝叶斯分类器提供了一个良好的基线来完成这个任务。\n",
    "    scikit-learn中包括这个分类器的许多变量，最适合进行单词计数的是多项式变量。\n",
    "\"\"\"\n",
    "from sklearn.naive_bayes import MultinomialNB  # 使用sklearn中的贝叶斯分类器，并且加载贝叶斯分类器\n",
    "\n",
    "# 中的MultinomialNB多项式函数\n",
    "clf = MultinomialNB()  # 加载多项式函数\n",
    "x_clf = clf.fit(X_train_tfidf, twenty_train.target)  # 构造基于数据的分类器\n",
    "print (x_clf)  # 分类器属性：MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
    "print ('-----')\n",
    "\"\"\"\n",
    "    为了预测输入的新的文档，我们需要使用与前面相同的特征提取链进行提取特征。\n",
    "    不同的是，在转换中，使用transform来代替fit_transform，因为训练集已经构造了分类器\n",
    "\"\"\"\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast']  # 文档\n",
    "X_new_counts = count_vect.transform(docs_new)  # 构建文档计数\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)  # 构建文档tfidf\n",
    "predicted = clf.predict(X_new_tfidf)  # 预测文档\n",
    "print (predicted)  # 预测类别 [3 1]，一个属于3类，一个属于1类\n",
    "for doc, category in zip(docs_new, predicted):\n",
    "    print ('%r => %s' % (doc, twenty_train.target_names[category]))  # 将文档和类别名字对应起来\n",
    "print ('-----')\n",
    "\"\"\"\n",
    "    Building a pipeline 建立管道\n",
    "    为了使向量转换更加简单(vectorizer => transformer => classifier)，scikit-learn提供了pipeline类来表示为一个复合分类器\n",
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "print (text_clf)  # 构造分类器，分类器的属性\n",
    "predicted = text_clf.predict(docs_new)  # 预测新文档\n",
    "print (predicted)  # 获取预测值\n",
    "print ('-----')\n",
    "\n",
    "\"\"\"\n",
    "    分析总结：\n",
    "        1、加载数据集，主要是加载训练集，用于对数据进行训练\n",
    "        2、文本特征提取：\n",
    "                对文本进行计数统计 CountVectorizer\n",
    "                词频统计  TfidfTransformer  （先计算tf,再计算tfidf）\n",
    "        3、训练分类器：\n",
    "                贝叶斯多项式训练器 MultinomialNB\n",
    "        4、预测文档：\n",
    "                通过构造的训练器进行构造分类器，来进行文档的预测\n",
    "        5、最简单的方式：\n",
    "                通过使用pipeline管道形式，来讲上述所有功能通过管道来一步实现，更加简单的就可以进行预测\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Evaluation of the performance on the test set 测试集性能评价\n",
    "    评估模型的预测精度同样容易：\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "docs_test = twenty_test.data\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print (np.mean(predicted == twenty_test.target))  # 预测的值和测试值的比例，mean就是比例函数\n",
    "print ('-----')  # 精度已经为0.834886817577\n",
    "\n",
    "\"\"\"\n",
    "    精度已经实现了83.4%，那么使用支持向量机(SVM)是否能够做的更好呢，支持向量机(SVM)被广泛认为是最好的文本分类算法之一。\n",
    "    尽管，SVM经常比贝叶斯要慢一些。\n",
    "    我们可以改变学习方式，使用管道来实现分类：\n",
    "\"\"\"\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf = Pipeline(\n",
    "    [('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
    "     ('clf', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42))])\n",
    "# _ = text_clf.fit(twenty_train.data, twenty_train.target)  # 和下面一句的意思一样，一个杠，表示本身\n",
    "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)\n",
    "predicted = text_clf.predict(docs_test)\n",
    "print (np.mean(predicted == twenty_test.target))  # 精度 0.912782956059\n",
    "print ('-----')\n",
    "\"\"\"\n",
    "    sklearn进一步提供了结果的更详细的性能分析工具：\n",
    "\"\"\"\n",
    "from sklearn import metrics\n",
    "print (metrics.classification_report(twenty_test.target, predicted, target_names=twenty_test.target_names))\n",
    "print (metrics.confusion_matrix(twenty_test.target, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
